{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snowflake-snowpark-python in c:\\python310\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: setuptools>=40.6.0 in c:\\python310\\lib\\site-packages (from snowflake-snowpark-python) (68.0.0)\n",
      "Requirement already satisfied: wheel in c:\\python310\\lib\\site-packages (from snowflake-snowpark-python) (0.37.1)\n",
      "Requirement already satisfied: cloudpickle<=2.0.0,>=1.6.0 in c:\\python310\\lib\\site-packages (from snowflake-snowpark-python) (2.0.0)\n",
      "Requirement already satisfied: snowflake-connector-python<4.0.0,>=2.7.12 in c:\\python310\\lib\\site-packages (from snowflake-snowpark-python) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.1.0 in c:\\python310\\lib\\site-packages (from snowflake-snowpark-python) (4.6.3)\n",
      "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (1.5.1)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (1.15.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: cryptography<41.0.0,>=3.1.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (40.0.2)\n",
      "Requirement already satisfied: oscrypto<2.0.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (1.3.0)\n",
      "Requirement already satisfied: pyOpenSSL<24.0.0,>=16.2.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (23.2.0)\n",
      "Requirement already satisfied: pycryptodomex!=3.5.0,<4.0.0,>=3.2 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (3.18.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (2.7.0)\n",
      "Requirement already satisfied: pytz in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (2022.1)\n",
      "Requirement already satisfied: requests<3.0.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (2.27.1)\n",
      "Requirement already satisfied: packaging in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (21.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (2021.10.8)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (3.12.2)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (2.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\python310\\lib\\site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\python310\\lib\\site-packages (from packaging->snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (3.0.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install snowflake-snowpark-python \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%I:%M:%S')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snowpark_session() -> Session:\n",
    "    connection_parameters = {\n",
    "       \"ACCOUNT\":\"ek87911.ap-southeast-1\",\n",
    "        \"USER\":\"snowpark_user\",\n",
    "        \"PASSWORD\":\"Test@12$4\",\n",
    "        \"ROLE\":\"SYSADMIN\",\n",
    "        \"DATABASE\":\"SNOWFLAKE_SAMPLE_DATA\",\n",
    "        \"SCHEMA\":\"TPCH_SF1\",\n",
    "        \"WAREHOUSE\":\"SNOWPARK_ETL_WH\"\n",
    "    }\n",
    "    return Session.builder.configs(connection_parameters).create()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:52:05 - INFO - Snowflake Connector for Python Version: 3.0.4, Python Version: 3.10.2, Platform: Windows-10-10.0.22621-SP0\n",
      "02:52:05 - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n",
      "02:52:05 - INFO - Setting use_openssl_only mode to False\n",
      "02:52:09 - INFO - Snowpark Session information: \n",
      "\"version\" : 1.5.1,\n",
      "\"python.version\" : 3.10.2,\n",
      "\"python.connector.version\" : 3.0.4,\n",
      "\"python.connector.session.id\" : 1418670647832610,\n",
      "\"os.name\" : Windows\n",
      "\n",
      "02:52:09 - INFO - query: [select current_role(), current_database(), current_schema(), current_warehouse()...]\n",
      "02:52:09 - INFO - query execution done\n",
      "02:52:09 - INFO - Number of results in first chunk: 0\n",
      "02:52:09 - INFO - query: [SELECT  *  FROM (select current_role(), current_database(), current_schema(), cu...]\n",
      "02:52:09 - INFO - query execution done\n",
      "02:52:09 - INFO - Number of results in first chunk: 1\n",
      "-----------------------------------------------------------------------------------------\n",
      "|\"CURRENT_ROLE()\"  |\"CURRENT_DATABASE()\"   |\"CURRENT_SCHEMA()\"  |\"CURRENT_WAREHOUSE()\"  |\n",
      "-----------------------------------------------------------------------------------------\n",
      "|SYSADMIN          |SNOWFLAKE_SAMPLE_DATA  |TPCH_SF1            |SNOWPARK_ETL_WH        |\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "02:52:09 - INFO - query: [select c_custkey,c_name,c_phone,c_mktsegment from snowflake_sample_data.tpch_sf1...]\n",
      "02:52:09 - INFO - query execution done\n",
      "02:52:09 - INFO - Number of results in first chunk: 0\n",
      "02:52:09 - INFO - query: [SELECT  *  FROM (select c_custkey,c_name,c_phone,c_mktsegment from snowflake_sam...]\n",
      "02:52:11 - INFO - query execution done\n",
      "02:52:11 - INFO - Number of results in first chunk: 5\n",
      "-----------------------------------------------------------------------\n",
      "|\"C_CUSTKEY\"  |\"C_NAME\"            |\"C_PHONE\"        |\"C_MKTSEGMENT\"  |\n",
      "-----------------------------------------------------------------------\n",
      "|60001        |Customer#000060001  |24-678-784-9652  |HOUSEHOLD       |\n",
      "|60002        |Customer#000060002  |25-782-500-8435  |BUILDING        |\n",
      "|60003        |Customer#000060003  |26-859-847-7640  |BUILDING        |\n",
      "|60004        |Customer#000060004  |20-573-674-7999  |AUTOMOBILE      |\n",
      "|60005        |Customer#000060005  |22-741-208-1316  |MACHINERY       |\n",
      "-----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    session = get_snowpark_session()\n",
    "\n",
    "    context_df = session.sql(\"select current_role(), current_database(), current_schema(), current_warehouse()\")\n",
    "    context_df.show(2)\n",
    "\n",
    "    customer_df = session.sql(\"select c_custkey,c_name,c_phone,c_mktsegment from snowflake_sample_data.tpch_sf1.customer limit 10\")\n",
    "    customer_df.show(5)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snowflake-connector-Python in c:\\python310\\lib\\site-packages (3.0.4)\n",
      "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-Python) (1.5.1)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in c:\\python310\\lib\\site-packages (from snowflake-connector-Python) (1.15.1)\n",
      "Requirement already satisfied: cryptography<41.0.0,>=3.1.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-Python) (40.0.2)\n",
      "Requirement already satisfied: oscrypto<2.0.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-Python) (1.3.0)\n",
      "Requirement already satisfied: pyOpenSSL<24.0.0,>=16.2.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-Python) (23.2.0)\n",
      "Requirement already satisfied: pycryptodomex!=3.5.0,<4.0.0,>=3.2 in c:\\python310\\lib\\site-packages (from snowflake-connector-Python) (3.18.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-Python) (2.7.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytz in c:\\python310\\lib\\site-packages (from snowflake-connector-Python) (2022.1)\n",
      "Requirement already satisfied: requests<3.0.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-Python) (2.27.1)\n",
      "Requirement already satisfied: packaging in c:\\python310\\lib\\site-packages (from snowflake-connector-Python) (21.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python310\\lib\\site-packages (from snowflake-connector-Python) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from snowflake-connector-Python) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python310\\lib\\site-packages (from snowflake-connector-Python) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from snowflake-connector-Python) (2021.10.8)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3 in c:\\python310\\lib\\site-packages (from snowflake-connector-Python) (4.6.3)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in c:\\python310\\lib\\site-packages (from snowflake-connector-Python) (3.12.2)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-Python) (2.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\python310\\lib\\site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-Python) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\python310\\lib\\site-packages (from packaging->snowflake-connector-Python) (3.0.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install snowflake-connector-Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snowflake-sqlalchemy in c:\\python310\\lib\\site-packages (1.4.7)\n",
      "Requirement already satisfied: sqlalchemy<2.0.0,>=1.4.0 in c:\\python310\\lib\\site-packages (from snowflake-sqlalchemy) (1.4.48)\n",
      "Requirement already satisfied: snowflake-connector-python<4.0.0 in c:\\python310\\lib\\site-packages (from snowflake-sqlalchemy) (3.0.4)\n",
      "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (1.5.1)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (1.15.1)\n",
      "Requirement already satisfied: cryptography<41.0.0,>=3.1.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (40.0.2)\n",
      "Requirement already satisfied: oscrypto<2.0.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (1.3.0)\n",
      "Requirement already satisfied: pyOpenSSL<24.0.0,>=16.2.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (23.2.0)\n",
      "Requirement already satisfied: pycryptodomex!=3.5.0,<4.0.0,>=3.2 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (3.18.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (2.7.0)\n",
      "Requirement already satisfied: pytz in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (2022.1)\n",
      "Requirement already satisfied: requests<3.0.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (2.27.1)\n",
      "Requirement already satisfied: packaging in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (21.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (2021.10.8)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (4.6.3)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (3.12.2)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (2.4.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\python310\\lib\\site-packages (from sqlalchemy<2.0.0,>=1.4.0->snowflake-sqlalchemy) (2.0.2)\n",
      "Requirement already satisfied: pycparser in c:\\python310\\lib\\site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\python310\\lib\\site-packages (from packaging->snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (3.0.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install snowflake-sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "Users\\LENOVO\\Downloads\\snowpark20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "def traverse_directory(directory,file_extension) -> list:\n",
    "    local_file_path = []\n",
    "    file_name = []  # List to store CSV file paths\n",
    "    partition_dir = []\n",
    "    print(directory)\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(file_extension):\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_name.append(file)\n",
    "                partition_dir.append(root.replace(directory, \"\"))\n",
    "                local_file_path.append(file_path)\n",
    "\n",
    "    return file_name,partition_dir,local_file_path\n",
    "\n",
    "def main():\n",
    "    # Specify the directory path to traverse\n",
    "    directory_path = \"Users\\LENOVO\\Downloads\\snowpark20\"\n",
    "    csv_file_name, csv_partition_dir , csv_local_file_path= traverse_directory(directory_path,'.csv')\n",
    "   # parquet_file_name, parquet_partition_dir , parquet_local_file_path= traverse_directory(directory_path,'.parquet')\n",
    "   # json_file_name, json_partition_dir , json_local_file_path= traverse_directory(directory_path,'.json')\n",
    "    stage_location = 'SALES_DWH.SOURCE.MY_INTERNAL_STG'\n",
    "\n",
    "    \n",
    "    csv_index = 0\n",
    "    for file_element in csv_file_name:\n",
    "        print(\"hi\")\n",
    "        put_result = ( \n",
    "                    get_snowpark_session().file.put( \n",
    "                        csv_local_file_path[csv_index], \n",
    "                        stage_location+\"/\"+csv_partition_dir[csv_index], \n",
    "                        auto_compress=False, overwrite=True, parallel=10)\n",
    "                    )\n",
    "        #put_result(file_element,\" => \",put_result[0].status)\n",
    "        csv_index+=1\n",
    "'''\n",
    "    parquet_index = 0\n",
    "    for file_element in parquet_file_name:\n",
    "\n",
    "        put_result = ( \n",
    "                    get_snowpark_session().file.put( \n",
    "                        parquet_local_file_path[parquet_index], \n",
    "                        stage_location+\"/\"+parquet_partition_dir[parquet_index], \n",
    "                        auto_compress=False, overwrite=True, parallel=10)\n",
    "                    )\n",
    "        #put_result(file_element,\" => \",put_result[0].status)\n",
    "        parquet_index+=1\n",
    "    \n",
    "    json_index = 0\n",
    "    for file_element in parquet_file_name:\n",
    "\n",
    "        put_result = ( \n",
    "                    get_snowpark_session().file.put( \n",
    "                        json_local_file_path[json_index], \n",
    "                        stage_location+\"/\"+json_partition_dir[json_index], \n",
    "                        auto_compress=False, overwrite=True, parallel=10)\n",
    "                    )\n",
    "        #put_result(file_element,\" => \",put_result[0].status)\n",
    "        json_index+=1  \n",
    "    '''\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    print(\"hi\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snowflake-snowpark-python in c:\\python310\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: setuptools>=40.6.0 in c:\\python310\\lib\\site-packages (from snowflake-snowpark-python) (68.0.0)\n",
      "Requirement already satisfied: wheel in c:\\python310\\lib\\site-packages (from snowflake-snowpark-python) (0.37.1)\n",
      "Requirement already satisfied: cloudpickle<=2.0.0,>=1.6.0 in c:\\python310\\lib\\site-packages (from snowflake-snowpark-python) (2.0.0)\n",
      "Requirement already satisfied: snowflake-connector-python<4.0.0,>=2.7.12 in c:\\python310\\lib\\site-packages (from snowflake-snowpark-python) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.1.0 in c:\\python310\\lib\\site-packages (from snowflake-snowpark-python) (4.6.3)\n",
      "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (1.5.1)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (1.15.1)\n",
      "Requirement already satisfied: cryptography<41.0.0,>=3.1.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (40.0.2)\n",
      "Requirement already satisfied: oscrypto<2.0.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (1.3.0)\n",
      "Requirement already satisfied: pyOpenSSL<24.0.0,>=16.2.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (23.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycryptodomex!=3.5.0,<4.0.0,>=3.2 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (3.18.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (2.7.0)\n",
      "Requirement already satisfied: pytz in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (2022.1)\n",
      "Requirement already satisfied: requests<3.0.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (2.27.1)\n",
      "Requirement already satisfied: packaging in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (21.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (2021.10.8)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (3.12.2)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in c:\\python310\\lib\\site-packages (from snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (2.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\python310\\lib\\site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\python310\\lib\\site-packages (from packaging->snowflake-connector-python<4.0.0,>=2.7.12->snowflake-snowpark-python) (3.0.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install snowflake-snowpark-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:52:25 - INFO - Snowflake Connector for Python Version: 3.0.4, Python Version: 3.10.2, Platform: Windows-10-10.0.22621-SP0\n",
      "02:52:25 - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n",
      "02:52:27 - INFO - Snowpark Session information: \n",
      "\"version\" : 1.5.1,\n",
      "\"python.version\" : 3.10.2,\n",
      "\"python.connector.version\" : 3.0.4,\n",
      "\"python.connector.session.id\" : 1418670647836714,\n",
      "\"os.name\" : Windows\n",
      "\n",
      "02:52:27 - INFO - query: [copy into sales_dwh.source.in_sales_order from (             select             ...]\n",
      "02:52:28 - INFO - query execution done\n",
      "02:52:29 - INFO - Number of results in first chunk: 1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "from snowflake.snowpark import Session, DataFrame\n",
    "from snowflake.snowpark.types import StructType, StringType, StructField, StringType,LongType,DecimalType,DateType,TimestampType\n",
    "from snowflake.snowpark.functions import col,lit,row_number, rank\n",
    "from snowflake.snowpark import Window\n",
    "\n",
    "# initiate logging at info level\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%I:%M:%S')\n",
    "\n",
    "# snowpark session\n",
    "def get_snowpark_session() -> Session:\n",
    "    connection_parameters = {\n",
    "       \"ACCOUNT\":\"ek87911.ap-southeast-1\",\n",
    "        \"USER\":\"snowpark_user\",\n",
    "        \"PASSWORD\":\"Test@12$4\",\n",
    "        \"ROLE\":\"SYSADMIN\",\n",
    "        \"DATABASE\":\"SNOWFLAKE_SAMPLE_DATA\",\n",
    "        \"SCHEMA\":\"TPCH_SF1\",\n",
    "        \"WAREHOUSE\":\"SNOWPARK_ETL_WH\"\n",
    "    }\n",
    "    return Session.builder.configs(connection_parameters).create()  \n",
    "\n",
    "def ingest_in_sales(session)-> None:\n",
    "    session.sql(\" \\\n",
    "            copy into sales_dwh.source.in_sales_order from ( \\\n",
    "            select \\\n",
    "            sales_dwh.source.in_sales_order_seq.nextval, \\\n",
    "            t.$1::text as order_id, \\\n",
    "            t.$2::text as customer_name, \\\n",
    "            t.$3::text as mobile_key,\\\n",
    "            t.$4::number as order_quantity, \\\n",
    "            t.$5::number as unit_price, \\\n",
    "            t.$6::number as order_valaue,  \\\n",
    "            t.$7::text as promotion_code , \\\n",
    "            t.$8::number(10,2)  as final_order_amount,\\\n",
    "            t.$9::number(10,2) as tax_amount,\\\n",
    "            t.$10::date as order_dt,\\\n",
    "            t.$11::text as payment_status,\\\n",
    "            t.$12::text as shipping_status,\\\n",
    "            t.$13::text as payment_method,\\\n",
    "            t.$14::text as payment_provider,\\\n",
    "            t.$15::text as mobile,\\\n",
    "            t.$16::text as shipping_address,\\\n",
    "            metadata$filename as stg_file_name,\\\n",
    "            metadata$file_row_number as stg_row_numer,\\\n",
    "            metadata$file_last_modified as stg_last_modified\\\n",
    "            from \\\n",
    "            @sales_dwh.source.my_internal_stg \\\n",
    "            (                                                             \\\n",
    "                file_format => 'sales_dwh.common.my_csv_format'           \\\n",
    "            ) t  )  on_error = 'Continue'     \\\n",
    "            \"\n",
    "            ).collect()\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    #get the session object and get dataframe\n",
    "    session = get_snowpark_session()\n",
    "\n",
    "    #ingest in sales data\n",
    "    ingest_in_sales(session)   \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:52:29 - INFO - Snowflake Connector for Python Version: 3.0.4, Python Version: 3.10.2, Platform: Windows-10-10.0.22621-SP0\n",
      "02:52:29 - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:52:30 - INFO - Snowpark Session information: \n",
      "\"version\" : 1.5.1,\n",
      "\"python.version\" : 3.10.2,\n",
      "\"python.connector.version\" : 3.0.4,\n",
      "\"python.connector.session.id\" : 1418670647832614,\n",
      "\"os.name\" : Windows\n",
      "\n",
      "02:52:30 - INFO - query: [select * from sales_dwh.source.in_sales_order]\n",
      "02:52:30 - INFO - query execution done\n",
      "02:52:30 - INFO - Number of results in first chunk: 0\n",
      "02:52:30 - INFO - query: [select * from sales_dwh.common.exchange_rate]\n",
      "02:52:30 - INFO - query execution done\n",
      "02:52:30 - INFO - Number of results in first chunk: 0\n",
      "02:52:30 - INFO - query: [SELECT \"SALES_ORDER_KEY\" AS \"SALES_ORDER_KEY\", \"ORDER_ID\" AS \"ORDER_ID\", \"CUSTOM...]\n",
      "02:52:31 - INFO - query execution done\n",
      "02:52:31 - INFO - Number of results in first chunk: 0\n",
      "02:52:31 - INFO - query: [SELECT \"DATE\" AS \"DATE\", \"USD2USD\" AS \"USD2USD\", \"USD2EU\" AS \"USD2EU\", \"USD2CAN\"...]\n",
      "02:52:31 - INFO - query execution done\n",
      "02:52:31 - INFO - Number of results in first chunk: 0\n",
      "02:52:31 - INFO - query: [SELECT  *  FROM (( SELECT NULL :: BIGINT AS \"SALES_ORDER_KEY\", NULL :: STRING(16...]\n",
      "02:52:31 - INFO - query execution done\n",
      "02:52:31 - INFO - Number of results in first chunk: 0\n",
      "02:52:31 - INFO - query: [SELECT \"SALES_ORDER_KEY\", \"ORDER_ID\", \"CUSTOMER_NAME\", \"MOBILE_KEY\", \"ORDER_QUAN...]\n",
      "02:52:31 - INFO - query execution done\n",
      "02:52:31 - INFO - Number of results in first chunk: 0\n",
      "02:52:31 - INFO - query: [SELECT \"UNIQUE_SALES_ORDER_KEY\" AS \"UNIQUE_SALES_ORDER_KEY\" FROM ( SELECT \"SALES...]\n",
      "02:52:31 - INFO - query execution done\n",
      "02:52:31 - INFO - Number of results in first chunk: 0\n",
      "02:52:31 - INFO - query: [SELECT \"SALES_ORDER_KEY\" AS \"SALES_ORDER_KEY\", \"ORDER_ID\" AS \"ORDER_ID\", \"CUSTOM...]\n",
      "02:52:31 - INFO - query execution done\n",
      "02:52:31 - INFO - Number of results in first chunk: 0\n",
      "02:52:31 - INFO - query: [SELECT  *  FROM (( SELECT NULL :: BIGINT AS \"UNIQUE_SALES_ORDER_KEY\") AS SNOWPAR...]\n",
      "02:52:31 - INFO - query execution done\n",
      "02:52:31 - INFO - Number of results in first chunk: 0\n",
      "02:52:31 - INFO - query: [show tables like 'in_sales_order' in schema sales_dwh.curated]\n",
      "02:52:32 - INFO - query execution done\n",
      "02:52:36 - INFO - Number of results in first chunk: 1\n",
      "02:52:36 - INFO - query: [INSERT  INTO sales_dwh.curated.in_sales_order SELECT  *  FROM ( SELECT \"SALES_OR...]\n",
      "02:52:38 - INFO - query execution done\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "from snowflake.snowpark import Session, DataFrame\n",
    "from snowflake.snowpark.functions import col,lit,row_number, rank\n",
    "from snowflake.snowpark import Window\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%I:%M:%S')\n",
    "\n",
    "\n",
    "def get_snowpark_session() -> Session:\n",
    "    connection_parameters = {\n",
    "       \"ACCOUNT\":\"ek87911.ap-southeast-1\",\n",
    "        \"USER\":\"snowpark_user\",\n",
    "        \"PASSWORD\":\"Test@12$4\",\n",
    "        \"ROLE\":\"SYSADMIN\",\n",
    "        \"DATABASE\":\"SNOWFLAKE_SAMPLE_DATA\",\n",
    "        \"SCHEMA\":\"TPCH_SF1\",\n",
    "        \"WAREHOUSE\":\"SNOWPARK_ETL_WH\"\n",
    "    }\n",
    "    return Session.builder.configs(connection_parameters).create()   \n",
    "\n",
    "def filter_dataset(df, column_name, filter_criterian) -> DataFrame:\n",
    "   \n",
    "    return_df = df.filter(col(column_name) == filter_criterian)\n",
    "\n",
    "    return return_df\n",
    "\n",
    "def main():\n",
    "\n",
    "   \n",
    "    session = get_snowpark_session()\n",
    "    sales_df = session.sql(\"select * from us_sales_order\")\n",
    "\n",
    "    paid_sales_df = filter_dataset(sales_df,'PAYMENT_STATUS','Paid')\n",
    "    shipped_sales_df = filter_dataset(paid_sales_df,'SHIPPING_STATUS','Delivered')\n",
    "\n",
    "    country_sales_df = shipped_sales_df.with_column('Country',lit('FR')).with_column('Region',lit('EU'))\n",
    "    forex_df = session.sql(\"select * from sales_dwh.common.exchange_rate\")\n",
    "    sales_with_forext_df = country_sales_df.join(forex_df,country_sales_df['order_dt']==forex_df['echange_rate_dt'],join_type='outer')\n",
    "\n",
    "    print(sales_with_forext_df.count())\n",
    "    unique_orders = sales_with_forext_df.with_column('order_rank',rank().over(Window.partitionBy(col(\"order_dt\")).order_by(col('_metadata_last_modified').desc()))).filter(col(\"order_rank\")==1).select(col('SALES_ORDER_KEY').alias('unique_sales_order_key'))\n",
    "    final_sales_df = unique_orders.join(sales_with_forext_df,unique_orders['unique_sales_order_key']==sales_with_forext_df['SALES_ORDER_KEY'],join_type='inner')\n",
    "    final_sales_df = final_sales_df.select(\n",
    "        col('SALES_ORDER_KEY'),\n",
    "        col('ORDER_ID'),\n",
    "        col('ORDER_DT'),\n",
    "        col('CUSTOMER_NAME'),\n",
    "        col('MOBILE_KEY'),\n",
    "        col('Country'),\n",
    "        col('Region'),\n",
    "        col('ORDER_QUANTITY'),\n",
    "        lit('EUR').alias('LOCAL_CURRENCY'),\n",
    "        col('UNIT_PRICE').alias('LOCAL_UNIT_PRICE'),\n",
    "        col('PROMOTION_CODE').alias('PROMOTION_CODE'),\n",
    "        col('FINAL_ORDER_AMOUNT').alias('LOCAL_TOTAL_ORDER_AMT'),\n",
    "        col('TAX_AMOUNT').alias('local_tax_amt'),\n",
    "        col('USD2FR').alias(\"Exhchange_Rate\"),\n",
    "        (col('FINAL_ORDER_AMOUNT')/col('USD2FR')).alias('US_TOTAL_ORDER_AMT'),\n",
    "        (col('TAX_AMOUNT')/col('USD2FR')).alias('USD_TAX_AMT'),\n",
    "        col('payment_status'),\n",
    "        col('shipping_status'),\n",
    "        col('payment_method'),\n",
    "        col('payment_provider'),\n",
    "        col('phone').alias('conctact_no'),\n",
    "        col('shipping_address')\n",
    "    )\n",
    "\n",
    "    final_sales_df.write.save_as_table(\"sales_dwh.curated.fr_sales_order\",mode=\"append\")\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:52:38 - INFO - Snowflake Connector for Python Version: 3.0.4, Python Version: 3.10.2, Platform: Windows-10-10.0.22621-SP0\n",
      "02:52:38 - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n",
      "02:52:39 - INFO - Snowpark Session information: \n",
      "\"version\" : 1.5.1,\n",
      "\"python.version\" : 3.10.2,\n",
      "\"python.connector.version\" : 3.0.4,\n",
      "\"python.connector.session.id\" : 1418670647836718,\n",
      "\"os.name\" : Windows\n",
      "\n",
      "02:52:39 - INFO - query: [select * from sales_dwh.curated.in_sales_order]\n",
      "02:52:40 - INFO - query execution done\n",
      "02:52:40 - INFO - Number of results in first chunk: 0\n",
      "02:52:40 - INFO - query: [SELECT min(\"ORDER_DT\") AS \"MIN_ORDER_DT\" FROM (select * from sales_dwh.curated.i...]\n",
      "02:52:40 - INFO - query execution done\n",
      "02:52:40 - INFO - Number of results in first chunk: 1\n",
      "02:52:40 - INFO - query: [SELECT max(\"ORDER_DT\") AS \"MAX_ORDER_DT\" FROM (select * from sales_dwh.curated.i...]\n",
      "02:52:40 - INFO - query execution done\n",
      "02:52:40 - INFO - Number of results in first chunk: 1\n"
     ]
    },
    {
     "ename": "MissingDependencyError",
     "evalue": "Missing optional dependency: pandas",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMissingDependencyError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\LENOVO\\Documents\\snowpython\\trial.ipynb Cell 11\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Documents/snowpython/trial.ipynb#X13sZmlsZQ%3D%3D?line=260'>261</a>\u001b[0m     all_sales_df\u001b[39m.\u001b[39mwrite\u001b[39m.\u001b[39msave_as_table(\u001b[39m\"\u001b[39m\u001b[39msales_dwh.consumption.sales_fact\u001b[39m\u001b[39m\"\u001b[39m,mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mappend\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Documents/snowpython/trial.ipynb#X13sZmlsZQ%3D%3D?line=263'>264</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Documents/snowpython/trial.ipynb#X13sZmlsZQ%3D%3D?line=264'>265</a>\u001b[0m     main()\n",
      "\u001b[1;32mc:\\Users\\LENOVO\\Documents\\snowpython\\trial.ipynb Cell 11\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Documents/snowpython/trial.ipynb#X13sZmlsZQ%3D%3D?line=215'>216</a>\u001b[0m fr_sales_df \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39msql(\u001b[39m\"\u001b[39m\u001b[39mselect * from sales_dwh.curated.fr_sales_order\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Documents/snowpython/trial.ipynb#X13sZmlsZQ%3D%3D?line=217'>218</a>\u001b[0m all_sales_df \u001b[39m=\u001b[39m in_sales_df\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Documents/snowpython/trial.ipynb#X13sZmlsZQ%3D%3D?line=219'>220</a>\u001b[0m create_date_dim(all_sales_df,session)       \u001b[39m#date dimension\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Documents/snowpython/trial.ipynb#X13sZmlsZQ%3D%3D?line=222'>223</a>\u001b[0m create_region_dim(all_sales_df,session)     \u001b[39m#region dimension\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Documents/snowpython/trial.ipynb#X13sZmlsZQ%3D%3D?line=223'>224</a>\u001b[0m create_product_dim(all_sales_df,session)    \u001b[39m#product dimension\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\LENOVO\\Documents\\snowpython\\trial.ipynb Cell 11\u001b[0m in \u001b[0;36mcreate_date_dim\u001b[1;34m(all_sales_df, session)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Documents/snowpython/trial.ipynb#X13sZmlsZQ%3D%3D?line=179'>180</a>\u001b[0m date_dim[\u001b[39m'\u001b[39m\u001b[39mDayOfMonth\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m date_range\u001b[39m.\u001b[39mday\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Documents/snowpython/trial.ipynb#X13sZmlsZQ%3D%3D?line=180'>181</a>\u001b[0m date_dim[\u001b[39m'\u001b[39m\u001b[39mWeekday\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m date_dim[\u001b[39m'\u001b[39m\u001b[39mDayOfWeek\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmap({\u001b[39m0\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mWeekday\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mWeekday\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m2\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mWeekday\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m3\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mWeekday\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m4\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mWeekday\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m5\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mWeekend\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m6\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mWeekend\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Documents/snowpython/trial.ipynb#X13sZmlsZQ%3D%3D?line=183'>184</a>\u001b[0m date_dim_df \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39;49mcreate_dataframe(date_dim)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Documents/snowpython/trial.ipynb#X13sZmlsZQ%3D%3D?line=185'>186</a>\u001b[0m existing_date_dim_df \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39msql(\u001b[39m\"\u001b[39m\u001b[39mselect order_dt from sales_dwh.consumption.date_dim \u001b[39m\u001b[39m\"\u001b[39m) \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Documents/snowpython/trial.ipynb#X13sZmlsZQ%3D%3D?line=186'>187</a>\u001b[0m date_dim_df \u001b[39m=\u001b[39m date_dim_df\u001b[39m.\u001b[39mjoin(existing_date_dim_df,existing_date_dim_df[\u001b[39m'\u001b[39m\u001b[39morder_dt\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m==\u001b[39mdate_dim_df[\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39morder_dt\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m],join_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleftanti\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\snowflake\\snowpark\\session.py:1473\u001b[0m, in \u001b[0;36mSession.create_dataframe\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m   1470\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Row):\n\u001b[0;32m   1471\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcreate_dataframe() function does not accept a Row object.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1473\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m, pandas\u001b[39m.\u001b[39;49mDataFrame)):\n\u001b[0;32m   1474\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   1475\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcreate_dataframe() function only accepts data as a list, tuple or a pandas DataFrame.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1476\u001b[0m     )\n\u001b[0;32m   1478\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m installed_pandas \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, pandas\u001b[39m.\u001b[39mDataFrame):\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\snowflake\\connector\\options.py:44\u001b[0m, in \u001b[0;36mMissingOptionalDependency.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, item):\n\u001b[1;32m---> 44\u001b[0m     \u001b[39mraise\u001b[39;00m errors\u001b[39m.\u001b[39mMissingDependencyError(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dep_name)\n",
      "\u001b[1;31mMissingDependencyError\u001b[0m: Missing optional dependency: pandas"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "from snowflake.snowpark import Session, DataFrame, CaseExpr\n",
    "from snowflake.snowpark.functions import col,lit,row_number, rank, split,cast, when, expr,min, max\n",
    "from snowflake.snowpark.types import StructType, StringType, StructField, StringType,LongType,DecimalType,DateType,TimestampType\n",
    "from snowflake.snowpark import Window\n",
    "\n",
    "# initiate logging at info level\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%I:%M:%S')\n",
    "\n",
    "# snowpark session\n",
    "def get_snowpark_session() -> Session:\n",
    "    connection_parameters = {\n",
    "       \"ACCOUNT\":\"ek87911.ap-southeast-1\",\n",
    "        \"USER\":\"snowpark_user\",\n",
    "        \"PASSWORD\":\"Test@12$4\",\n",
    "        \"ROLE\":\"SYSADMIN\",\n",
    "        \"DATABASE\":\"SNOWFLAKE_SAMPLE_DATA\",\n",
    "        \"SCHEMA\":\"TPCH_SF1\",\n",
    "        \"WAREHOUSE\":\"SNOWPARK_ETL_WH\"\n",
    "    }\n",
    "    return Session.builder.configs(connection_parameters).create()   \n",
    "\n",
    "# This is a simple dim table having nation and region.\n",
    "# fields are 'Country','Region'\n",
    "def create_region_dim(all_sales_df,session)-> None:\n",
    "    region_dim_df = all_sales_df.groupBy(col(\"Country\"),col(\"Region\")).count()\n",
    "    region_dim_df.show(2)\n",
    "    region_dim_df = region_dim_df.with_column(\"isActive\",lit('Y'))\n",
    "    region_dim_df = region_dim_df.selectExpr(\"sales_dwh.source.region_dim_seq.nextval as region_id_pk\",\"Country\", \"Region\", \"isActive\") \n",
    "    #region_dim_df.write.save_as_table('sales_dwh.consumption.region_dim',mode=\"append\")   \n",
    "\n",
    "    region_dim_df.show(5)\n",
    "    # part 2 where delta data will be processed \n",
    "    \n",
    "    existing_region_dim_df = session.sql(\"select Country, Region from sales_dwh.consumption.region_dim\")\n",
    "\n",
    "    region_dim_df = region_dim_df.join(existing_region_dim_df,region_dim_df['Country']==existing_region_dim_df['Country'],join_type='leftanti')\n",
    "    region_dim_df.show(5)\n",
    "    intsert_cnt = int(region_dim_df.count())\n",
    "    if intsert_cnt>0:\n",
    "        region_dim_df.write.save_as_table(\"sales_dwh.consumption.region_dim\",mode=\"append\")\n",
    "        print(\"save operation ran...\")\n",
    "    else:\n",
    "        print(\"No insert ...Opps...\")\n",
    "\n",
    "\n",
    "    # have exclude key\n",
    "\n",
    "\n",
    "def create_product_dim(all_sales_df,session)-> None:\n",
    "\n",
    "    product_dim_df = all_sales_df.with_column(\"Brand\",split(col('MOBILE_KEY'),lit('/'))[0])     \\\n",
    "                                .with_column(\"Model\",split(col('MOBILE_KEY'),lit('/'))[1])      \\\n",
    "                                .with_column(\"Color\",split(col('MOBILE_KEY'),lit('/'))[2])      \\\n",
    "                                .with_column(\"Memory\",split(col('MOBILE_KEY'),lit('/'))[3])     \\\n",
    "                                .select(col('mobile_key'),col('Brand'),col('Model'),col('Color'),col('Memory'))\n",
    "    \n",
    "    product_dim_df = product_dim_df.select(col('mobile_key'),                    \\\n",
    "                                        cast(col('Brand'), StringType()).as_(\"Brand\"),\\\n",
    "                                        cast(col('Model'), StringType()).as_(\"Model\"),\\\n",
    "                                        cast(col('Color'), StringType()).as_(\"Color\"),\\\n",
    "                                        cast(col('Memory'), StringType()).as_(\"Memory\")\\\n",
    "                                        )\n",
    "    \n",
    "    product_dim_df = product_dim_df.groupBy(col('mobile_key'),col(\"Brand\"),col(\"Model\"),col(\"Color\"),col(\"Memory\")).count()\n",
    "    product_dim_df = product_dim_df.with_column(\"isActive\",lit('Y'))\n",
    "\n",
    "    #fetch existing product dim records.\n",
    "    existing_product_dim_df = session.sql(\"select mobile_key, Brand, Model, Color, Memory from sales_dwh.consumption.product_dim\")\n",
    "    existing_product_dim_df.count()\n",
    "\n",
    "    product_dim_df = product_dim_df.join(existing_product_dim_df,[\"mobile_key\", \"Brand\", \"Model\", \"Color\", \"Memory\"],join_type='leftanti')\n",
    "\n",
    "    product_dim_df.show(5)\n",
    "\n",
    "    product_dim_df = product_dim_df.selectExpr(\"sales_dwh.consumption.product_dim_seq.nextval as product_id_pk\",\"mobile_key\",\"Brand\", \"Model\",\"Color\",\"Memory\", \"isActive\") \n",
    "\n",
    "    product_dim_df.show(5)\n",
    "    intsert_cnt = int(product_dim_df.count())\n",
    "    if intsert_cnt>0:\n",
    "        product_dim_df.write.save_as_table(\"sales_dwh.consumption.product_dim\",mode=\"append\")\n",
    "        print(\"save operation ran...\")\n",
    "    else:\n",
    "        print(\"No insert ...Opps...\")\n",
    "\n",
    "def create_promocode_dim(all_sales_df,session)-> None:\n",
    "\n",
    "\n",
    "    promo_code_dim_df = all_sales_df.with_column( \"promotion_code\", expr(\"case when promotion_code is null then 'NA' else promotion_code end\"))\n",
    "    promo_code_dim_df = promo_code_dim_df.groupBy(col(\"promotion_code\"),col(\"country\"),col(\"region\")).count()\n",
    "    promo_code_dim_df = promo_code_dim_df.with_column(\"isActive\",lit('Y'))\n",
    "\n",
    "    #promo_code_dim_df.show(10)\n",
    "\n",
    "    \n",
    "    #fetch existing product dim records.\n",
    "    existing_promo_code_dim_df = session.sql(\"select promotion_code, country, region from sales_dwh.consumption.promo_code_dim\")\n",
    "\n",
    "\n",
    "    promo_code_dim_df = promo_code_dim_df.join(existing_promo_code_dim_df,[\"promotion_code\", \"country\", \"region\"],join_type='leftanti')\n",
    "\n",
    "\n",
    "    promo_code_dim_df = promo_code_dim_df.selectExpr(\"sales_dwh.consumption.promo_code_dim_seq.nextval as promo_code_id_pk\",\"promotion_code\", \"country\",\"region\",\"isActive\") \n",
    "\n",
    "\n",
    "    intsert_cnt = int(promo_code_dim_df.count())\n",
    "    if intsert_cnt>0:\n",
    "        promo_code_dim_df.write.save_as_table(\"sales_dwh.consumption.promo_code_dim\",mode=\"append\")\n",
    "        print(\"save operation ran...\")\n",
    "    else:\n",
    "        print(\"No insert ...Opps...\")\n",
    "    \n",
    "def create_customer_dim(all_sales_df, session) -> None:\n",
    "    customer_dim_df = all_sales_df.groupBy(col(\"COUNTRY\"),col(\"REGION\"),col(\"CUSTOMER_NAME\"),col(\"CONCTACT_NO\"),col(\"SHIPPING_ADDRESS\")).count()\n",
    "    customer_dim_df = customer_dim_df.with_column(\"isActive\",lit('Y'))\n",
    "    customer_dim_df = customer_dim_df.selectExpr(\"customer_name\", \"conctact_no\",\"shipping_address\",\"country\",\"region\" ,\"isactive\") \n",
    "    #region_dim_df.write.save_as_table('sales_dwh.consumption.region_dim',mode=\"append\")   \n",
    "\n",
    "    customer_dim_df.show(5)\n",
    "    # part 2 where delta data will be processed \n",
    "    \n",
    "    existing_customer_dim_df = session.sql(\"select customer_name,conctact_no,shipping_address,country, region from sales_dwh.consumption.customer_dim\")\n",
    "\n",
    "    customer_dim_df = customer_dim_df.join(existing_customer_dim_df,[\"customer_name\",\"conctact_no\",\"shipping_address\",\"country\", \"region\"],join_type='leftanti')\n",
    "\n",
    "    customer_dim_df = customer_dim_df.selectExpr(\"sales_dwh.consumption.customer_dim_seq.nextval as customer_id_pk\",\"customer_name\", \"conctact_no\",\"shipping_address\",\"country\",\"region\", \"isActive\") \n",
    "\n",
    "    customer_dim_df.show(5)\n",
    "\n",
    "    intsert_cnt = int(customer_dim_df.count())\n",
    "    if intsert_cnt>0:\n",
    "        customer_dim_df.write.save_as_table(\"sales_dwh.consumption.customer_dim\",mode=\"append\")\n",
    "        print(\"save operation ran...\")\n",
    "    else:\n",
    "        print(\"No insert ...Opps...\")\n",
    "    \n",
    "def create_payment_dim(all_sales_df, session) -> None:\n",
    "    payment_dim_df = all_sales_df.groupBy(col(\"COUNTRY\"),col(\"REGION\"),col(\"payment_method\"),col(\"payment_provider\")).count()\n",
    "    payment_dim_df = payment_dim_df.with_column(\"isActive\",lit('Y'))\n",
    "\n",
    "    #region_dim_df.write.save_as_table('sales_dwh.consumption.region_dim',mode=\"append\")   \n",
    "\n",
    "    payment_dim_df.show(5)\n",
    "    # part 2 where delta data will be processed \n",
    "    \n",
    "    existing_payment_dim_df = session.sql(\"select payment_method,payment_provider,country, region from sales_dwh.consumption.payment_dim\")\n",
    "\n",
    "    payment_dim_df = payment_dim_df.join(existing_payment_dim_df,[\"payment_method\",\"payment_provider\",\"country\", \"region\"],join_type='leftanti')\n",
    "\n",
    "    payment_dim_df = payment_dim_df.selectExpr(\"sales_dwh.consumption.payment_dim_seq.nextval as payment_id_pk\",\"payment_method\", \"payment_provider\",\"country\",\"region\", \"isActive\") \n",
    "\n",
    "\n",
    "    intsert_cnt = int(payment_dim_df.count())\n",
    "    if intsert_cnt>0:\n",
    "        payment_dim_df.write.save_as_table(\"sales_dwh.consumption.payment_dim\",mode=\"append\")\n",
    "        print(\"save operation ran...\")\n",
    "    else:\n",
    "        print(\"No insert ...Opps...\")\n",
    "\n",
    "def create_date_dim(all_sales_df, session) -> None:\n",
    "    start_date = all_sales_df.select(min(\"order_dt\").alias(\"min_order_dt\")).collect()[0].as_dict()['MIN_ORDER_DT']\n",
    "    end_date = all_sales_df.select(max(\"order_dt\").alias(\"max_order_dt\")).collect()[0].as_dict()['MAX_ORDER_DT']\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    #print(date_range)\n",
    "    date_dim = pd.DataFrame()\n",
    "    date_dim['order_dt'] = date_range.date\n",
    "    date_dim['Year'] = date_range.year\n",
    "    # Calculate day counter\n",
    "    start_day_of_year = pd.to_datetime(start_date).dayofyear\n",
    "    date_dim['DayCounter'] = date_range.dayofyear - start_day_of_year + 1\n",
    "\n",
    "    date_dim['Month'] = date_range.month\n",
    "    date_dim['Quarter'] = date_range.quarter\n",
    "    date_dim['Day'] = date_range.day\n",
    "    date_dim['DayOfWeek'] = date_range.dayofweek\n",
    "    date_dim['DayName'] = date_range.strftime('%A')\n",
    "    date_dim['DayOfMonth'] = date_range.day\n",
    "    date_dim['Weekday'] = date_dim['DayOfWeek'].map({0: 'Weekday', 1: 'Weekday', 2: 'Weekday', 3: 'Weekday', 4: 'Weekday', 5: 'Weekend', 6: 'Weekend'})\n",
    "\n",
    "\n",
    "    date_dim_df = session.create_dataframe(date_dim)\n",
    "\n",
    "    existing_date_dim_df = session.sql(\"select order_dt from sales_dwh.consumption.date_dim \") \n",
    "    date_dim_df = date_dim_df.join(existing_date_dim_df,existing_date_dim_df['order_dt']==date_dim_df['\"order_dt\"'],join_type='leftanti')\n",
    "\n",
    "    date_dim_df = date_dim_df.selectExpr(' \\\n",
    "                                   sales_dwh.consumption.date_dim_seq.nextval, \\\n",
    "                                   \"order_dt\" as order_dt, \\\n",
    "                                   \"DayCounter\" as day_counter,\\\n",
    "                                   \"Year\" as order_year, \\\n",
    "                                   \"Month\" as order_month, \\\n",
    "                                   \"Quarter\" as order_quarter, \\\n",
    "                                   \"Day\" as order_day, \\\n",
    "                                   \"DayOfWeek\" as order_dayofweek, \\\n",
    "                                   \"DayName\" as order_dayname, \\\n",
    "                                   \"DayOfMonth\" as order_dayofmonth, \\\n",
    "                                   \"Weekday\" as order_weekday\\\n",
    "                                   ')\n",
    "\n",
    "    intsert_cnt = int(date_dim_df.count())\n",
    "    if intsert_cnt>0:\n",
    "        date_dim_df.write.save_as_table(\"sales_dwh.consumption.date_dim\",mode=\"append\")\n",
    "        print(\"save operation ran...\")\n",
    "    else:\n",
    "        print(\"No insert ...Opps...\")\n",
    "\n",
    "def main():\n",
    "    #get the session object and get dataframe\n",
    "    session = get_snowpark_session()\n",
    "\n",
    "    in_sales_df = session.sql(\"select * from sales_dwh.curated.in_sales_order\")\n",
    "    us_sales_df = session.sql(\"select * from sales_dwh.curated.us_sales_order\")\n",
    "    fr_sales_df = session.sql(\"select * from sales_dwh.curated.fr_sales_order\")\n",
    "\n",
    "    all_sales_df = in_sales_df\n",
    "\n",
    "    create_date_dim(all_sales_df,session)       #date dimension\n",
    "\n",
    "\n",
    "    create_region_dim(all_sales_df,session)     #region dimension\n",
    "    create_product_dim(all_sales_df,session)    #product dimension\n",
    "    create_promocode_dim(all_sales_df,session)  #promot code dimension\n",
    "    create_customer_dim(all_sales_df,session)   #customer dimension\n",
    "    create_payment_dim(all_sales_df,session)    #payment dimension\n",
    "    create_date_dim(all_sales_df,session)       #date dimension\n",
    "\n",
    "\n",
    "    date_dim_df = session.sql(\"select date_id_pk, order_dt from sales_dwh.consumption.date_dim\")\n",
    "    customer_dim_df = session.sql(\"select customer_id_pk, customer_name, country, region from sales_dwh.consumption.CUSTOMER_DIM\")\n",
    "    payment_dim_df = session.sql(\"select payment_id_pk, payment_method, payment_provider, country, region from sales_dwh.consumption.PAYMENT_DIM\")\n",
    "    product_dim_df = session.sql(\"select product_id_pk, mobile_key from sales_dwh.consumption.PRODUCT_DIM\")\n",
    "    promo_code_dim_df = session.sql(\"select promo_code_id_pk,promotion_code,country, region from sales_dwh.consumption.PROMO_CODE_DIM\")\n",
    "    region_dim_df = session.sql(\"select region_id_pk,country, region from sales_dwh.consumption.REGION_DIM\")\n",
    "\n",
    "    all_sales_df = all_sales_df.with_column( \"promotion_code\", expr(\"case when promotion_code is null then 'NA' else promotion_code end\"))\n",
    "    all_sales_df = all_sales_df.join(date_dim_df, [\"order_dt\"],join_type='inner')\n",
    "    all_sales_df = all_sales_df.join(customer_dim_df, [\"customer_name\",\"region\",\"country\"],join_type='inner')\n",
    "    all_sales_df = all_sales_df.join(payment_dim_df, [\"payment_method\", \"payment_provider\", \"country\", \"region\"],join_type='inner')\n",
    "    #all_sales_df = all_sales_df.join(product_dim_df, [\"brand\",\"model\",\"color\",\"Memory\"],join_type='inner')\n",
    "    all_sales_df = all_sales_df.join(product_dim_df, [\"mobile_key\"],join_type='inner')\n",
    "    all_sales_df = all_sales_df.join(promo_code_dim_df, [\"promotion_code\",\"country\", \"region\"],join_type='inner')\n",
    "    all_sales_df = all_sales_df.join(region_dim_df, [\"country\", \"region\"],join_type='inner')\n",
    "    all_sales_df = all_sales_df.selectExpr(\"sales_dwh.consumption.sales_fact_seq.nextval as order_id_pk, \\\n",
    "                                           order_id as order_code,                               \\\n",
    "                                           date_id_pk as date_id_fk,          \\\n",
    "                                           region_id_pk as region_id_fk,            \\\n",
    "                                           customer_id_pk as customer_id_fk,        \\\n",
    "                                           payment_id_pk as payment_id_fk,          \\\n",
    "                                           product_id_pk as product_id_fk,          \\\n",
    "                                           promo_code_id_pk as promo_code_id_fk,    \\\n",
    "                                           order_quantity,                          \\\n",
    "                                           local_total_order_amt,                   \\\n",
    "                                           local_tax_amt,                           \\\n",
    "                                           exhchange_rate,                          \\\n",
    "                                           us_total_order_amt,                      \\\n",
    "                                           usd_tax_amt                              \\\n",
    "                                           \")\n",
    "    all_sales_df.write.save_as_table(\"sales_dwh.consumption.sales_fact\",mode=\"append\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
